{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "o443ZHri8R1F",
        "outputId": "0b4d548a-a66f-491a-e354-35b9f811b0b9"
      },
      "outputs": [],
      "source": [
        "!pip install deep-translator\n",
        "!pip install pandas langdetect"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6L69ilzl7ufq",
        "outputId": "0e1d3313-c7fd-4bcf-cc00-f36fb6abc1dd"
      },
      "outputs": [],
      "source": [
        "import requests\n",
        "from bs4 import BeautifulSoup\n",
        "import csv\n",
        "import re\n",
        "from deep_translator import GoogleTranslator\n",
        "from langdetect import detect\n",
        "\n",
        "BASE_URL  = \"https://repositorio.upch.edu.pe\"\n",
        "START_URL = f\"{BASE_URL}/handle/20.500.12866/1318/recent-submissions\"\n",
        "#MAX_ARTICULOS = 20  # Cambia si deseas más artículos\n",
        "MAX_ARTICULOS = float('inf')\n",
        "datos = []\n",
        "\n",
        "def limpia_texto(t):\n",
        "    return re.sub(r'\\s+', ' ', t).strip()\n",
        "\n",
        "def traducir(texto):\n",
        "    if texto.strip() == \"\" or texto == \"NA\":\n",
        "        return \"NA\"\n",
        "    try:\n",
        "        return GoogleTranslator(source='auto', target='en').translate(texto)\n",
        "    except Exception as e:\n",
        "        print(f\"Error al traducir: {e}\")\n",
        "        return \"NA\"\n",
        "\n",
        "def traducir_si_espanol(texto):\n",
        "    if texto.strip() == \"\" or texto == \"NA\":\n",
        "        return \"NA\"\n",
        "    try:\n",
        "        if detect(texto) == 'es':\n",
        "            return traducir(texto)\n",
        "        return texto  # Ya está en inglés u otro idioma\n",
        "    except Exception as e:\n",
        "        print(f\"Error detectando idioma: {e}\")\n",
        "        return texto  # Devuelve el original si no se puede detectar\n",
        "\n",
        "def extraer_resumen_completo(url):\n",
        "    try:\n",
        "        r = requests.get(url)\n",
        "        soup = BeautifulSoup(r.text, \"html.parser\")\n",
        "        resumen_div = soup.select_one(\"div.simple-item-view-description\")\n",
        "\n",
        "        if not resumen_div:\n",
        "            return \"NA\"\n",
        "\n",
        "        # Convertir el HTML interno a texto dividido por los spacers\n",
        "        raw_html = str(resumen_div)\n",
        "        partes = raw_html.split('<div class=\"spacer\">')\n",
        "\n",
        "        if len(partes) < 3:\n",
        "            # Si no hay al menos dos separadores, devuelve todo el texto\n",
        "            texto_completo = BeautifulSoup(raw_html, \"html.parser\").get_text(\" \", strip=True)\n",
        "            return limpia_texto(re.sub(r'^Resumen[:：]?\\s*', '', texto_completo, flags=re.IGNORECASE))\n",
        "\n",
        "        # La parte en inglés está entre el primer y segundo spacer => partes[1]\n",
        "        texto_ingles_html = partes[1]\n",
        "        texto_ingles = BeautifulSoup(texto_ingles_html, \"html.parser\").get_text(\" \", strip=True)\n",
        "        texto_ingles = limpia_texto(re.sub(r'^Resumen[:：]?\\s*', '', texto_ingles, flags=re.IGNORECASE))\n",
        "\n",
        "        return texto_ingles if texto_ingles.strip() else \"NA\"\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"Error al extraer resumen en {url}: {e}\")\n",
        "        return \"NA\"\n",
        "\n",
        "def scrape_listing_page(url):\n",
        "    global datos\n",
        "    print(f\"Scraping: {url}\")\n",
        "    r = requests.get(url)\n",
        "    soup = BeautifulSoup(r.text, \"html.parser\")\n",
        "    items = soup.select(\"li.ds-artifact-item\")\n",
        "\n",
        "    \"\"\"\n",
        "    for item in items:\n",
        "        if len(datos) >= MAX_ARTICULOS:\n",
        "            return\n",
        "    \"\"\"\n",
        "    for item in items:\n",
        "\n",
        "        titulo_tag    = item.select_one(\"div.artifact-title a\")\n",
        "        autores_tag   = item.select_one(\"span.author\")\n",
        "        fecha_tag     = item.select_one(\"span.date\")\n",
        "        publisher_tag = item.select_one(\"span.publisher\")\n",
        "\n",
        "        titulo    = limpia_texto(titulo_tag.text) if titulo_tag else \"NA\"\n",
        "        link      = BASE_URL + titulo_tag[\"href\"] if titulo_tag else \"NA\"\n",
        "        autores   = limpia_texto(autores_tag.text) if autores_tag else \"NA\"\n",
        "        fecha     = limpia_texto(fecha_tag.text) if fecha_tag else \"NA\"\n",
        "        editorial = limpia_texto(publisher_tag.text) if publisher_tag else \"NA\"\n",
        "        resumen   = extraer_resumen_completo(link) if link != \"NA\" else \"NA\"\n",
        "\n",
        "        print(f\"Traduciendo artículo {len(datos)+1}: {titulo[:50]}...\")\n",
        "        datos.append({\n",
        "            \"title\": traducir_si_espanol(titulo),\n",
        "            \"abstract\": traducir_si_espanol(resumen),\n",
        "            \"publisher\": traducir_si_espanol(editorial),\n",
        "            \"authors\": autores,\n",
        "            \"date\": fecha,\n",
        "            \"link\": link  # No se traduce\n",
        "        })\n",
        "\n",
        "    # Recorrer siguiente página si existe\n",
        "    next_btn = soup.select_one(\"a.next-page-link\")\n",
        "    if next_btn and next_btn.get(\"href\") and len(datos) < MAX_ARTICULOS:\n",
        "        scrape_listing_page(BASE_URL + next_btn[\"href\"])\n",
        "\n",
        "# Ejecutar scraping y traducción\n",
        "scrape_listing_page(START_URL)\n",
        "\n",
        "# Guardar resultados traducidos en formato compatible con Excel en español (delimitador ;)\n",
        "with open(\"articulos_upch_traducidos_con_editorial.csv\", \"w\", newline=\"\", encoding=\"utf-8\") as f:\n",
        "    fieldnames = [\"title\", \"abstract\", \"publisher\", \"authors\", \"date\", \"link\"]\n",
        "    writer = csv.DictWriter(f, fieldnames=fieldnames, delimiter=';', quoting=csv.QUOTE_ALL)\n",
        "    writer.writeheader()\n",
        "    writer.writerows(datos)\n",
        "\n",
        "print(f\"\\n Se guardaron {len(datos)} artículos traducidos en 'articulos_upch_traducidos_con_editorial.csv'\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 17
        },
        "id": "GoAl5Jq89RRi",
        "outputId": "4efa1b96-a406-4094-933b-a61a48cfd06f"
      },
      "outputs": [],
      "source": [
        "from google.colab import files\n",
        "files.download(\"articulos_upch_traducidos_con_editorial.csv\")"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
